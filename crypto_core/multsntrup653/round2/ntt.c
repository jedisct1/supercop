#include "crypto_core.h"
#include "ntt.h"

/* auto-generated; do not edit */

#include <stdint.h>
#include <immintrin.h>

typedef int8_t int8;
typedef int16_t int16;

#define zeta(n,i) (i[(__m256i *) zeta_##n])
#define zeta_x4(n,i) (i[(__m256i *) zeta_x4_##n])
#define zeta_qinv(n,i) (i[(__m256i *) qinvzeta_##n])
#define zeta_x4_qinv(n,i) (i[(__m256i *) qinvzeta_x4_##n])
#define zetainv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) zeta_##n+n/2+1-16*(i+1)))
#define zetainv_x4(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) zeta_x4_##n+2*n+4-16*(i+1)))
#define zetainv_qinv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) qinvzeta_##n+n/2+1-16*(i+1)))
#define zetainv_x4_qinv(n,i) _mm256_loadu_reverse16((__m256i *) ((int16 *) qinvzeta_x4_##n+2*n+4-16*(i+1)))

static const __attribute((aligned(32))) int16 qdata_7681[] = {

#define q_x16 (qdata[0])
  7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,7681,

#define qrecip_x16 (qdata[1])
  17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,17474,

#define qshift_x16 (qdata[2])
  16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,

#define zeta4_x16 (qdata[3])
  -3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,-3777,

#define zeta4_x16_qinv (qdata[4])
  -28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,-28865,

#define zeta8_x16 (qdata[5])
  -3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,-3625,

#define zeta8_x16_qinv (qdata[6])
  -16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,-16425,

#define zetainv8_x16 (qdata[7])
  -3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,-3182,

#define zetainv8_x16_qinv (qdata[8])
  -10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,-10350,

#define zeta_x4_16 (qdata+9)
  -3593,-3593,-3593,-3593,-2194,-2194,-2194,-2194,-3625,-3625,-3625,-3625,1100,1100,1100,1100,
  -3777,-3777,-3777,-3777,-2456,-2456,-2456,-2456,3182,3182,3182,3182,3696,3696,3696,3696,
  3593,3593,3593,3593,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_x4_16 (qdata+12)
  -9,-9,-9,-9,4974,4974,4974,4974,-16425,-16425,-16425,-16425,7244,7244,7244,7244,
  -28865,-28865,-28865,-28865,-14744,-14744,-14744,-14744,10350,10350,10350,10350,-4496,-4496,-4496,-4496,
  9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_x4_32 (qdata+15)
  -3593,-3593,-3593,-3593,1414,1414,1414,1414,-2194,-2194,-2194,-2194,-2495,-2495,-2495,-2495,
  -3625,-3625,-3625,-3625,2876,2876,2876,2876,1100,1100,1100,1100,-2250,-2250,-2250,-2250,
  -3777,-3777,-3777,-3777,-1701,-1701,-1701,-1701,-2456,-2456,-2456,-2456,834,834,834,834,
  3182,3182,3182,3182,-2319,-2319,-2319,-2319,3696,3696,3696,3696,121,121,121,121,
  3593,3593,3593,3593,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_x4_32 (qdata+20)
  -9,-9,-9,-9,20870,20870,20870,20870,4974,4974,4974,4974,22593,22593,22593,22593,
  -16425,-16425,-16425,-16425,828,828,828,828,7244,7244,7244,7244,-23754,-23754,-23754,-23754,
  -28865,-28865,-28865,-28865,20315,20315,20315,20315,-14744,-14744,-14744,-14744,18242,18242,18242,18242,
  10350,10350,10350,10350,-18191,-18191,-18191,-18191,-4496,-4496,-4496,-4496,-11655,-11655,-11655,-11655,
  9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_64 (qdata+25)
  -3593,-617,1414,3706,-2194,-1296,-2495,-2237,-3625,2830,2876,-1599,1100,1525,-2250,2816,
  -3777,1921,-1701,2006,-2456,1483,834,-1986,3182,3364,-2319,-1993,3696,-2557,121,2088,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_64 (qdata+28)
  -9,19351,20870,-15750,4974,-9488,22593,7491,-16425,26382,828,23489,7244,20469,-23754,2816,
  -28865,-5759,20315,-3114,-14744,15307,18242,-19394,10350,-10972,-18191,-31177,-4496,-25597,-11655,22568,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_128 (qdata+31)
  -3593,-2804,-617,-396,1414,-549,3706,810,-2194,-1321,-1296,438,-2495,-2535,-2237,-3689,
  -3625,2043,2830,-1881,2876,3153,-1599,7,1100,-514,1525,-1760,-2250,-2440,2816,3600,
  -3777,103,1921,-3174,-1701,1535,2006,-1887,-2456,1399,1483,-679,834,3772,-1986,1738,
  3182,-1431,3364,-3555,-2319,-2310,-1993,638,3696,-2956,-2557,-1305,121,2555,2088,-3266,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_128 (qdata+36)
  -9,-29428,19351,26228,20870,21467,-15750,5930,4974,-14121,-9488,-21066,22593,2073,7491,16279,
  -16425,-25093,26382,26279,828,-29103,23489,11783,7244,14846,20469,14624,-23754,-6536,2816,11792,
  -28865,-4505,-5759,-6246,20315,9215,-3114,6817,-14744,4983,15307,-28839,18242,1724,-19394,23242,
  10350,-21399,-10972,-29667,-18191,-21766,-31177,15998,-4496,23668,-25597,-5913,-11655,-24581,22568,-20674,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_256 (qdata+41)
  -3593,2665,-2804,-2572,-617,727,-396,3417,1414,2579,-549,373,3706,3750,810,-1054,
  -2194,-2133,-1321,1681,-1296,-1386,438,-2732,-2495,1919,-2535,-2391,-2237,2835,-3689,2,
  -3625,-783,2043,3145,2830,1533,-1881,2789,2876,2649,3153,3692,-1599,-1390,7,-1166,
  1100,3310,-514,2224,1525,-2743,-1760,2385,-2250,-486,-2440,-1756,2816,-3816,3600,-3831,
  -3777,-1799,103,1497,1921,1521,-3174,-194,-1701,-859,1535,2175,2006,-2762,-1887,-1698,
  -2456,-3480,1399,2883,1483,-3428,-679,-2113,834,1532,3772,-660,-1986,-2764,1738,-915,
  3182,1056,-1431,1350,3364,1464,-3555,2919,-2319,-2160,-2310,730,-1993,-1598,638,3456,
  3696,-1168,-2956,-3588,-2557,-921,-1305,3405,121,-404,2555,-3135,2088,2233,-3266,-2426,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_256 (qdata+50)
  -9,-17303,-29428,24052,19351,-12073,26228,-24743,20870,-12269,21467,19317,-15750,-25946,5930,32738,
  4974,-4693,-14121,2193,-9488,26262,-21066,7508,22593,9599,2073,10409,7491,-12013,16279,-15358,
  -16425,-16655,-25093,32329,26382,24573,26279,13541,828,-25511,-29103,26220,23489,-8558,11783,-24718,
  7244,10478,14846,26800,20469,26441,14624,-29871,-23754,-3558,-6536,-16092,2816,8472,11792,-7415,
  -28865,-13575,-4505,-26663,-5759,-14351,-6246,-17602,20315,-22875,9215,9855,-3114,-24266,6817,-2722,
  -14744,-15768,4983,12611,15307,-21860,-28839,-27201,18242,32252,1724,21868,-19394,-8908,23242,13933,
  10350,17440,-21399,-11962,-10972,30136,-29667,-1689,-18191,6032,-21766,30426,-31177,15810,15998,3456,
  -4496,-9360,23668,27132,-25597,-5529,-5913,1869,-11655,22124,-24581,21953,22568,23225,-20674,17030,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define zeta_512 (qdata+59)
  -3593,2005,2665,2891,-2804,2345,-2572,1121,-617,-188,727,2786,-396,-3208,3417,-17,
  1414,-3752,2579,2815,-549,1837,373,151,3706,-1012,3750,-1509,810,-3214,-1054,3177,
  -2194,-1403,-2133,-3314,-1321,83,1681,-658,-1296,2070,-1386,-3547,438,3781,-2732,2230,
  -2495,-1669,1919,2589,-2535,-3312,-2391,-3542,-2237,-1441,2835,-3568,-3689,-402,2,-1070,
  -3625,3763,-783,-3550,2043,-2303,3145,-436,2830,-893,1533,1712,-1881,124,2789,-2001,
  2876,-2460,2649,3770,3153,2965,3692,-1203,-1599,2874,-1390,-1407,7,-3745,-1166,1649,
  1100,2937,3310,3461,-514,-1526,2224,715,1525,-1689,-2743,434,-1760,-3163,2385,-929,
  -2250,-2167,-486,-1144,-2440,-370,-1756,2378,2816,-1084,-3816,-1586,3600,1931,-3831,-1242,
  -3777,592,-1799,2340,103,-1338,1497,-2071,1921,1519,1521,451,-3174,589,-194,-3744,
  -1701,3677,-859,-1295,1535,642,2175,-3794,2006,2130,-2762,2918,-1887,3334,-1698,2072,
  -2456,509,-3480,2998,1399,-3408,2883,1476,1483,-2262,-3428,-1779,-679,2258,-2113,1348,
  834,-692,1532,2247,3772,2083,-660,-226,-1986,2532,-2764,-3693,1738,-429,-915,-2059,
  3182,2812,1056,3434,-1431,-2515,1350,-236,3364,-2386,1464,222,-3555,-2963,2919,-2422,
  -2319,-3657,-2160,3450,-2310,-791,730,1181,-1993,-1404,-1598,2339,638,-3366,3456,2161,
  3696,-3343,-1168,2719,-2956,-826,-3588,-670,-2557,777,-921,1151,-1305,-796,3405,-1278,
  121,-3287,-404,1072,2555,293,-3135,2767,2088,-3335,2233,3581,-3266,3723,-2426,-179,
  3593,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

#define qinvzeta_512 (qdata+76)
  -9,4565,-17303,16715,-29428,15145,24052,-22943,19351,1860,-12073,-28958,26228,-7304,-24743,-529,
  20870,-24232,-12269,10495,21467,-16083,19317,20119,-15750,-27636,-25946,-12261,5930,-26766,32738,-16791,
  4974,25733,-4693,20238,-14121,18003,2193,6510,-9488,29718,26262,-25563,-21066,-1851,7508,-19274,
  22593,-28805,9599,-23523,2073,4880,10409,1578,7491,-10145,-12013,4624,16279,6766,-15358,24530,
  -16425,5299,-16655,-2526,-25093,-9983,32329,5708,26382,-23933,24573,26288,26279,30844,13541,30255,
  828,15972,-25511,17082,-29103,-27243,26220,-2739,23489,16186,-8558,-9087,11783,-12449,-24718,-14223,
  7244,-8839,10478,30597,14846,-12790,26800,14539,20469,-6297,26441,9650,14624,-25179,-29871,-9633,
  -23754,-5751,-3558,2952,-6536,23182,-16092,23882,2816,964,8472,-10802,11792,-17013,-7415,-30938,
  -28865,-23984,-13575,-11996,-4505,-14650,-26663,-22039,-5759,1007,-14351,10179,-6246,-947,-17602,-20128,
  20315,10333,-22875,-17167,9215,-14718,9855,-29394,-3114,27730,-24266,5990,6817,22790,-2722,14360,
  -14744,23549,-15768,-18506,4983,21168,12611,3524,15307,2858,-21860,29453,-28839,27858,-27201,3396,
  18242,5452,32252,-18745,1724,-4573,21868,31518,-19394,20964,-8908,-18541,23242,17491,13933,16885,
  10350,-32004,17440,-24214,-21399,-20435,-11962,-22764,-10972,-27986,30136,-802,-29667,11885,-1689,-13686,
  -18191,32695,6032,-16006,-21766,-20759,30426,-24931,-31177,-32124,15810,-4317,15998,26330,3456,-13711,
  -4496,-19215,-9360,26783,23668,-14138,27132,-32414,-25597,-2807,-5529,8831,-5913,17636,1869,-16638,
  -11655,9513,22124,25648,-24581,-21723,21953,-14129,22568,-15111,23225,26621,-20674,-15221,17030,-1715,
  9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

};

static const __attribute((aligned(32))) int16 qdata_10753[] = {

  10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,10753,

  24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,24964,

  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,

  223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,

  27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,27359,

  4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,4188,

  -1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,-1956,

  3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,3688,

  -408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,-408,

  1018,1018,1018,1018,2413,2413,2413,2413,4188,4188,4188,4188,357,357,357,357,
  223,223,223,223,-3686,-3686,-3686,-3686,-3688,-3688,-3688,-3688,-376,-376,-376,-376,
  -1018,-1018,-1018,-1018,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-6,-6,-6,10093,10093,10093,10093,-1956,-1956,-1956,-1956,28517,28517,28517,28517,
  27359,27359,27359,27359,-21094,-21094,-21094,-21094,408,408,408,408,-20856,-20856,-20856,-20856,
  6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,1018,1018,1018,-2695,-2695,-2695,-2695,2413,2413,2413,2413,425,425,425,425,
  4188,4188,4188,4188,-4855,-4855,-4855,-4855,357,357,357,357,-3364,-3364,-3364,-3364,
  223,223,223,223,730,730,730,730,-3686,-3686,-3686,-3686,-4544,-4544,-4544,-4544,
  -3688,-3688,-3688,-3688,-2236,-2236,-2236,-2236,-376,-376,-376,-376,3784,3784,3784,3784,
  -1018,-1018,-1018,-1018,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-6,-6,-6,7033,7033,7033,7033,10093,10093,10093,10093,18345,18345,18345,18345,
  -1956,-1956,-1956,-1956,29449,29449,29449,29449,28517,28517,28517,28517,-9508,-9508,-9508,-9508,
  27359,27359,27359,27359,16090,16090,16090,16090,-21094,-21094,-21094,-21094,28224,28224,28224,28224,
  408,408,408,408,-12476,-12476,-12476,-12476,-20856,-20856,-20856,-20856,16072,16072,16072,16072,
  6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-1520,-2695,1341,2413,918,425,5175,4188,-4035,-4855,341,357,4347,-3364,5213,
  223,-4875,730,1931,-3686,-2503,-4544,-4095,-3688,5063,-2236,-3823,-376,3012,3784,-2629,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,23056,7033,829,10093,26518,18345,3639,-1956,-4547,29449,3925,28517,-7429,-9508,-11683,
  27359,-17675,16090,14731,-21094,-25543,28224,-14847,408,28103,-12476,10001,-20856,-7228,16072,18363,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-2935,-1520,-4744,-2695,-205,1341,1299,2413,4,918,-4379,425,-4616,5175,-544,
  4188,4129,-4035,4102,-4855,-1287,341,-2388,357,1284,4347,2984,-3364,2178,5213,-2576,
  223,2790,-4875,4876,730,-4513,1931,-3085,-3686,3550,-2503,847,-4544,193,-4095,1085,
  -3688,3091,5063,-4742,-2236,2982,-3823,-1009,-376,-268,3012,3062,3784,-2565,-2629,4189,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,31369,23056,15736,7033,-24269,829,-6381,10093,22532,26518,23781,18345,15864,3639,15840,
  -1956,-23007,-4547,5126,29449,8441,3925,-16724,28517,23812,-7429,31656,-9508,-19326,-11683,-27152,
  27359,20198,-17675,6924,16090,22623,14731,5619,-21094,-24098,-25543,3407,28224,22209,-14847,573,
  408,-4589,28103,-5766,-12476,-12378,10001,-31217,-20856,-2316,-7228,-20490,16072,-14341,18363,-12707,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,-4734,-2935,-400,-1520,4977,-4744,-2973,-2695,512,-205,-779,1341,-1356,1299,635,
  2413,567,4,-4286,918,-5114,-4379,-1586,425,1615,-4616,-336,5175,-1841,-544,2234,
  4188,-3441,4129,636,-4035,-4580,4102,2684,-4855,3057,-1287,-2740,341,-5156,-2388,-472,
  357,-794,1284,578,4347,3615,2984,-3715,-3364,2271,2178,-326,5213,454,-2576,-3337,
  223,2998,2790,-151,-4875,2981,4876,1324,730,2774,-4513,2206,1931,886,-3085,-970,
  -3686,3198,3550,2737,-2503,-909,847,1068,-4544,-2213,193,2884,-4095,-4808,1085,4123,
  -3688,5341,3091,5294,5063,-116,-4742,-5116,-2236,-2045,2982,-1572,-3823,4828,-1009,467,
  -376,5023,-268,-3169,3012,-1458,3062,-1268,3784,-675,-2565,1006,-2629,5064,4189,864,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-26238,31369,-24976,23056,-30351,15736,-18845,7033,512,-24269,-13579,829,29364,-6381,-11141,
  10093,-969,22532,6978,26518,-4090,23781,11726,18345,4175,15864,7856,3639,719,15840,-31558,
  -1956,31887,-23007,-21892,-4547,22044,5126,-19844,29449,-32271,8441,32076,3925,-11300,-16724,28200,
  28517,16614,23812,11842,-7429,-2017,31656,28541,-9508,29407,-19326,31418,-11683,-31290,-27152,27895,
  27359,12214,20198,-14999,-17675,-1627,6924,-13012,16090,-4394,22623,7326,14731,-22666,5619,8246,
  -21094,24702,-24098,177,-25543,7795,3407,-13268,28224,2395,22209,-7356,-14847,-17096,573,-24037,
  408,-11555,-4589,-30546,28103,1932,-5766,17412,-12476,31235,-12378,-7716,10001,-1316,-31217,25555,
  -20856,-609,-2316,-8801,-7228,11854,-20490,780,16072,-17571,-14341,-2066,18363,17352,-12707,17248,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  1018,3453,-4734,4519,-2935,2118,-400,-554,-1520,2196,4977,1893,-4744,-1409,-2973,-1053,
  -2695,4601,512,279,-205,-3241,-779,4889,1341,3524,-1356,-1663,1299,2283,635,73,
  2413,2428,567,624,4,-1930,-4286,3419,918,-2062,-5114,5068,-4379,-97,-1586,1782,
  425,4621,1615,355,-4616,1349,-336,825,5175,3135,-1841,1160,-544,4408,2234,-2605,
  4188,854,-3441,-1056,4129,2439,636,4967,-4035,-4782,-4580,-5268,4102,-663,2684,-4670,
  -4855,3760,3057,3535,-1287,2680,-2740,-569,341,2139,-5156,3827,-2388,1639,-472,1927,
  357,5172,-794,-4003,1284,4144,578,693,4347,4784,3615,3125,2984,1122,-3715,2113,
  -3364,-573,2271,-4328,2178,2909,-326,-4000,5213,-4447,454,-3995,-2576,-4428,-3337,2529,
  223,5309,2998,5120,2790,-2050,-151,2963,-4875,2657,2981,-2807,4876,2237,1324,-4403,
  730,2624,2774,-5083,-4513,40,2206,152,1931,-1573,886,2625,-3085,-778,-970,-5107,
  -3686,4250,3198,-5356,3550,-3148,2737,-3360,-2503,-2015,-909,3096,847,5313,1068,834,
  -4544,-1132,-2213,-2151,193,-1722,2884,-4393,-4095,2662,-4808,-2788,1085,-1992,4123,5334,
  -3688,5215,5341,-1689,3091,-2117,5294,4859,5063,3410,-116,2205,-4742,-2374,-5116,-4720,
  -2236,3570,-2045,2813,2982,2087,-1572,-4973,-3823,458,4828,3891,-1009,-2419,467,-4891,
  -376,-1381,5023,1204,-268,274,-3169,-3260,3012,-1635,-1458,4540,3062,-4254,-1268,-1111,
  3784,2230,-675,-2279,-2565,-4359,1006,-1510,-2629,5015,5064,-2449,4189,-5005,864,2487,
  -1018,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

  -6,-29827,-26238,-21593,31369,-29626,-24976,-7722,23056,-16236,-30351,30053,15736,9343,-18845,-16925,
  7033,14329,512,15127,-24269,-21161,-13579,-1767,829,-6716,29364,-12415,-6381,31467,-11141,1609,
  10093,-20100,-969,-23952,22532,-25482,6978,8027,26518,17394,-4090,-25652,23781,-5729,11726,-21770,
  18345,-4083,4175,-15517,15864,-19643,7856,-22215,3639,-18881,719,-19320,15840,-7880,-31558,22483,
  -1956,-6314,31887,15328,-23007,-7289,-21892,11623,-4547,31058,22044,13164,5126,-15511,-19844,6594,
  29449,11952,-32271,6095,8441,23160,32076,22471,3925,6747,-11300,12531,-16724,8295,28200,-7801,
  28517,-29644,16614,-20899,23812,12336,11842,20661,-7429,12976,-2017,23093,31656,-3998,28541,24129,
  -9508,-61,29407,-232,-19326,-13987,31418,12384,-11683,-31583,-31290,24165,-27152,26292,27895,8161,
  27359,4797,12214,5120,20198,19454,-14999,-4717,-17675,8289,-1627,31497,6924,1725,-13012,19661,
  16090,-30144,-4394,-9691,22623,28712,7326,4248,14731,3035,-22666,24641,5619,-24330,8246,-13811,
  -21094,-13158,24702,-23788,-24098,27572,177,13024,-25543,-29151,7795,7192,3407,27329,-13268,12098,
  28224,-19564,2395,-8807,22209,32070,-7356,-22313,-14847,20070,-17096,23836,573,-14280,-24037,-1834,
  408,32351,-11555,4967,-4589,18875,-30546,-6917,28103,-26286,1932,18077,-5766,29370,17412,19856,
  -12476,23026,31235,-30467,-12378,-24025,-7716,-12653,10001,-8758,-1316,-20173,-31217,-11123,25555,23269,
  -20856,-29541,-609,31924,-2316,3346,-8801,-13500,-7228,14237,11854,14780,-20490,-9374,780,16809,
  16072,11446,-17571,-8935,-14341,5369,-2066,-18918,18363,19863,17352,-16273,-12707,3699,17248,951,
  6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

};

static inline __m256i sub_x16(__m256i a,__m256i b)
{
  __asm__("vpsubw %1,%0,%0" : "+x"(a),"+x"(b));
  return a;
}

static inline __m256i add_x16(__m256i a,__m256i b)
{
  return _mm256_add_epi16(a,b);
}

static inline __m256i reduce_x16(const __m256i *qdata,__m256i x)
{
  __m256i y = _mm256_mulhi_epi16(x,qrecip_x16);
  y = _mm256_mulhrs_epi16(y,qshift_x16);
  y = _mm256_mullo_epi16(y,q_x16);
  return sub_x16(x,y);
}

static inline __m256i mulmod_x16_scaled(const __m256i *qdata,__m256i x,__m256i y,__m256i yqinv)
{
  __m256i b = _mm256_mulhi_epi16(x,y);
  __m256i d = _mm256_mullo_epi16(x,yqinv);
  __m256i e = _mm256_mulhi_epi16(d,q_x16);
  return sub_x16(b,e);
}

static __attribute((aligned(32))) int8 shuffle_buf[32] = {
  14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,
  14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,
};
#define shuffle (*(__m256i *) shuffle_buf)

static inline __m256i _mm256_loadu_reverse16(const __m256i *p)
{
  __m256i x = _mm256_loadu_si256(p);
  x = _mm256_permute2x128_si256(x,x,1);
  x = _mm256_shuffle_epi8(x,shuffle);
  return x;
}

static void ntt128(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  __m256i zetainv_128_0 = zetainv(128,0);
  __m256i zetainv_qinv_128_0 = zetainv_qinv(128,0);
  __m256i zetainv_x4_32_0 = zetainv_x4(32,0);
  __m256i zetainv_x4_qinv_32_0 = zetainv_x4_qinv(32,0);
  __m256i zetainv_128_1 = zetainv(128,1);
  __m256i zetainv_qinv_128_1 = zetainv_qinv(128,1);
  __m256i zetainv_x4_32_1 = zetainv_x4(32,1);
  __m256i zetainv_x4_qinv_32_1 = zetainv_x4_qinv(32,1);
  for (rep = 0;rep < reps;++rep) {
    f1 = _mm256_loadu_si256((__m256i *) (f + 32));
    f3 = _mm256_loadu_si256((__m256i *) (f + 96));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 64));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_128_0,zetainv_qinv_128_0);
    f2 = mulmod_x16_scaled(qdata,f2,zeta(128,0),zeta_qinv(128,0));
    
    g2 = _mm256_unpacklo_epi16(f2,f3);
    g3 = _mm256_unpackhi_epi16(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta(64,0),zeta_qinv(64,0));
    f0 = reduce_x16(qdata,f0);
    
    g0 = _mm256_unpacklo_epi16(f0,f1);
    h0 = _mm256_unpacklo_epi32(g0,g2);
    h1 = _mm256_unpackhi_epi32(g0,g2);
    g1 = _mm256_unpackhi_epi16(f0,f1);
    h2 = _mm256_unpacklo_epi32(g1,g3);
    h3 = _mm256_unpackhi_epi32(g1,g3);
    f0 = _mm256_permute2x128_si256(h0,h1,0x20);
    f2 = _mm256_permute2x128_si256(h0,h1,0x31);
    f1 = _mm256_permute2x128_si256(h2,h3,0x20);
    f3 = _mm256_permute2x128_si256(h2,h3,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 64),f2);
    _mm256_storeu_si256((__m256i *) (f + 32),f1);
    _mm256_storeu_si256((__m256i *) (f + 96),f3);
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 48));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 16));
    f2 = _mm256_loadu_si256((__m256i *) (f + 80));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_128_1,zetainv_qinv_128_1);
    f2 = mulmod_x16_scaled(qdata,f2,zeta(128,1),zeta_qinv(128,1));
    
    g2 = _mm256_unpacklo_epi16(f2,f3);
    g3 = _mm256_unpackhi_epi16(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta(64,1),zeta_qinv(64,1));
    f0 = reduce_x16(qdata,f0);
    
    g0 = _mm256_unpacklo_epi16(f0,f1);
    h0 = _mm256_unpacklo_epi32(g0,g2);
    h1 = _mm256_unpackhi_epi32(g0,g2);
    g1 = _mm256_unpackhi_epi16(f0,f1);
    h2 = _mm256_unpacklo_epi32(g1,g3);
    h3 = _mm256_unpackhi_epi32(g1,g3);
    f0 = _mm256_permute2x128_si256(h0,h1,0x20);
    f2 = _mm256_permute2x128_si256(h0,h1,0x31);
    f1 = _mm256_permute2x128_si256(h2,h3,0x20);
    f3 = _mm256_permute2x128_si256(h2,h3,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 16),f0);
    _mm256_storeu_si256((__m256i *) (f + 80),f2);
    _mm256_storeu_si256((__m256i *) (f + 48),f1);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f1 = _mm256_loadu_si256((__m256i *) (f + 64));
    f3 = _mm256_loadu_si256((__m256i *) (f + 80));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 16));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_x4_32_0,zetainv_x4_qinv_32_0);
    f2 = mulmod_x16_scaled(qdata,f2,zeta_x4(32,0),zeta_x4_qinv(32,0));
    
    g2 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta_x4(16,0),zeta_x4_qinv(16,0));
    f0 = reduce_x16(qdata,f0);
    
    g1 = _mm256_unpackhi_epi64(f0,f1);
    g0 = _mm256_unpacklo_epi64(f0,f1);
    f1 = _mm256_permute2x128_si256(g1,g3,0x20);
    f3 = _mm256_permute2x128_si256(g1,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g2,0x20);
    f2 = _mm256_permute2x128_si256(g0,g2,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 64),f1);
    _mm256_storeu_si256((__m256i *) (f + 80),f3);
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 16),f2);
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 96));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 32));
    f2 = _mm256_loadu_si256((__m256i *) (f + 48));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv_x4_32_1,zetainv_x4_qinv_32_1);
    f2 = mulmod_x16_scaled(qdata,f2,zeta_x4(32,1),zeta_x4_qinv(32,1));
    
    g2 = _mm256_unpacklo_epi64(f2,f3);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta_x4(16,1),zeta_x4_qinv(16,1));
    f0 = reduce_x16(qdata,f0);
    
    g1 = _mm256_unpackhi_epi64(f0,f1);
    g0 = _mm256_unpacklo_epi64(f0,f1);
    f1 = _mm256_permute2x128_si256(g1,g3,0x20);
    f3 = _mm256_permute2x128_si256(g1,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g2,0x20);
    f2 = _mm256_permute2x128_si256(g0,g2,0x31);
    
    _mm256_storeu_si256((__m256i *) (f + 96),f1);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    _mm256_storeu_si256((__m256i *) (f + 32),f0);
    _mm256_storeu_si256((__m256i *) (f + 48),f2);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 16));
    f3 = _mm256_loadu_si256((__m256i *) (f + 48));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 32));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f2 = add_x16(g2,g3);
    f3 = sub_x16(g2,g3);
    f2 = reduce_x16(qdata,f2);
    f3 = reduce_x16(qdata,f3);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f0 = reduce_x16(qdata,f0);
    
    h0 = f0;
    h1 = f1;
    h2 = f2;
    h3 = f3;
    
    f1 = _mm256_loadu_si256((__m256i *) (f + 80));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    g3 = sub_x16(f1,f3);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f1,f3);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 64));
    f2 = _mm256_loadu_si256((__m256i *) (f + 96));
    g2 = sub_x16(f0,f2);
    g0 = add_x16(f0,f2);
    
    f3 = sub_x16(g3,g2);
    f2 = add_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zetainv8_x16,zetainv8_x16_qinv);
    f2 = mulmod_x16_scaled(qdata,f2,zeta8_x16,zeta8_x16_qinv);
    
    f1 = sub_x16(g0,g1);
    f0 = add_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zeta4_x16,zeta4_x16_qinv);
    f0 = reduce_x16(qdata,f0);
    
    g0 = add_x16(h0,f0);
    g1 = add_x16(h1,f1);
    g2 = add_x16(h2,f2);
    g3 = add_x16(h3,f3);
    _mm256_storeu_si256((__m256i *) (f + 0),g0);
    _mm256_storeu_si256((__m256i *) (f + 16),g1);
    _mm256_storeu_si256((__m256i *) (f + 32),g2);
    _mm256_storeu_si256((__m256i *) (f + 48),g3);
    g0 = sub_x16(h0,f0);
    g1 = sub_x16(h1,f1);
    g2 = sub_x16(h2,f2);
    g3 = sub_x16(h3,f3);
    _mm256_storeu_si256((__m256i *) (f + 64),g0);
    _mm256_storeu_si256((__m256i *) (f + 80),g1);
    _mm256_storeu_si256((__m256i *) (f + 96),g2);
    _mm256_storeu_si256((__m256i *) (f + 112),g3);
    f += 128;
  }
}

static void ntt512(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  __m256i zetainv_512[8];
  __m256i zetainv_qinv_512[8];
  int i;
  for (i = 0;i < 8;++i) zetainv_512[i] = zetainv(512,i);
  for (i = 0;i < 8;++i) zetainv_qinv_512[i] = zetainv_qinv(512,i);
  for (rep = 0;rep < reps;++rep) {
    for (i = 0;i < 8;++i) {
      f1 = _mm256_loadu_si256((__m256i *) (f + 16*i + 128));
      f3 = _mm256_loadu_si256((__m256i *) (f + 16*i + 384));
      g3 = sub_x16(f1,f3);
      g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
      g1 = add_x16(f1,f3);
      
      f0 = _mm256_loadu_si256((__m256i *) (f + 16*i));
      f2 = _mm256_loadu_si256((__m256i *) (f + 16*i + 256));
      g2 = sub_x16(f0,f2);
      g0 = add_x16(f0,f2);
      
      f3 = sub_x16(g3,g2);
      f2 = add_x16(g2,g3);
      f3 = mulmod_x16_scaled(qdata,f3,zetainv_512[i],zetainv_qinv_512[i]);
      f2 = mulmod_x16_scaled(qdata,f2,zeta(512,i),zeta_qinv(512,i));
      
      f1 = sub_x16(g0,g1);
      f0 = add_x16(g0,g1);
      f1 = mulmod_x16_scaled(qdata,f1,zeta(256,i),zeta_qinv(256,i));
      f0 = reduce_x16(qdata,f0);
      
      _mm256_storeu_si256((__m256i *) (f + 16*i + 384),f3);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 256),f2);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 128),f1);
      _mm256_storeu_si256((__m256i *) (f + 16*i),f0);
      
    }
    f += 512;
  }
  f = origf;
  ntt128(f,reps*4,qdata);
}

void ntt512_7681(int16 *f,int reps)
{
  ntt512(f,reps,(const __m256i *) qdata_7681);
}

void ntt512_10753(int16 *f,int reps)
{
  ntt512(f,reps,(const __m256i *) qdata_10753);
}

static void invntt128(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  __m256i zetainv_x4_16_0 = zetainv_x4(16,0);
  __m256i zetainv_x4_qinv_16_0 = zetainv_x4_qinv(16,0);
  __m256i zetainv_x4_32_0 = zetainv_x4(32,0);
  __m256i zetainv_x4_qinv_32_0 = zetainv_x4_qinv(32,0);
  __m256i zetainv_64_0 = zetainv(64,0);
  __m256i zetainv_qinv_64_0 = zetainv_qinv(64,0);
  __m256i zetainv_128_0 = zetainv(128,0);
  __m256i zetainv_qinv_128_0 = zetainv_qinv(128,0);
  __m256i zetainv_x4_16_1 = zetainv_x4(16,1);
  __m256i zetainv_x4_qinv_16_1 = zetainv_x4_qinv(16,1);
  __m256i zetainv_x4_32_1 = zetainv_x4(32,1);
  __m256i zetainv_x4_qinv_32_1 = zetainv_x4_qinv(32,1);
  __m256i zetainv_64_1 = zetainv(64,1);
  __m256i zetainv_qinv_64_1 = zetainv_qinv(64,1);
  __m256i zetainv_128_1 = zetainv(128,1);
  __m256i zetainv_qinv_128_1 = zetainv_qinv(128,1);
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f +   0));
    f1 = _mm256_loadu_si256((__m256i *) (f +  64));
    f2 = _mm256_loadu_si256((__m256i *) (f +  16));
    f3 = _mm256_loadu_si256((__m256i *) (f +  80));
    g0 = _mm256_loadu_si256((__m256i *) (f +  32));
    g1 = _mm256_loadu_si256((__m256i *) (f +  96));
    g2 = _mm256_loadu_si256((__m256i *) (f +  48));
    g3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    h1 = sub_x16(f0,f1);
    h1 = reduce_x16(qdata,h1);
    h0 = add_x16(f0,f1);
    h3 = sub_x16(f2,f3);
    h3 = mulmod_x16_scaled(qdata,h3,zeta4_x16,zeta4_x16_qinv);
    h2 = add_x16(f2,f3);
    f1 = sub_x16(g0,g1);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv8_x16,zetainv8_x16_qinv);
    f0 = add_x16(g0,g1);
    f3 = sub_x16(g2,g3);
    f3 = mulmod_x16_scaled(qdata,f3,zeta8_x16,zeta8_x16_qinv);
    f2 = add_x16(g2,g3);
    
    g0 = add_x16(h0,h2);
    g0 = reduce_x16(qdata,g0);
    g2 = sub_x16(h0,h2);
    g2 = reduce_x16(qdata,g2);
    g1 = sub_x16(h1,h3);
    g3 = add_x16(h1,h3);
    h2 = sub_x16(f0,f2);
    h2 = mulmod_x16_scaled(qdata,h2,zeta4_x16,zeta4_x16_qinv);
    h0 = add_x16(f0,f2);
    h3 = add_x16(f1,f3);
    h3 = mulmod_x16_scaled(qdata,h3,zeta4_x16,zeta4_x16_qinv);
    h1 = sub_x16(f1,f3);
    
    f0 = add_x16(g0,h0);
    g0 = sub_x16(g0,h0);
    f1 = add_x16(g1,h1);
    g1 = sub_x16(g1,h1);
    f2 = sub_x16(g2,h2);
    g2 = add_x16(g2,h2);
    f3 = sub_x16(g3,h3);
    g3 = add_x16(g3,h3);
    
    _mm256_storeu_si256((__m256i *) (f +   0),f0);
    _mm256_storeu_si256((__m256i *) (f +  32),g0);
    _mm256_storeu_si256((__m256i *) (f +  64),f1);
    _mm256_storeu_si256((__m256i *) (f +  96),g1);
    _mm256_storeu_si256((__m256i *) (f +  16),f2);
    _mm256_storeu_si256((__m256i *) (f +  48),g2);
    _mm256_storeu_si256((__m256i *) (f +  80),f3);
    _mm256_storeu_si256((__m256i *) (f + 112),g3);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f1 = _mm256_loadu_si256((__m256i *) (f + 64));
    f2 = _mm256_loadu_si256((__m256i *) (f + 16));
    f3 = _mm256_loadu_si256((__m256i *) (f + 80));
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g1 = _mm256_unpacklo_epi64(f2,f3);
    g2 = _mm256_unpackhi_epi64(f0,f1);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    f2 = _mm256_permute2x128_si256(g0,g1,0x31);
    f3 = _mm256_permute2x128_si256(g2,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g1,0x20);
    f1 = _mm256_permute2x128_si256(g2,g3,0x20);
    
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_x4_32_0,zetainv_x4_qinv_32_0);
    f3 = mulmod_x16_scaled(qdata,f3,zeta_x4(32,0),zeta_x4_qinv(32,0));
    
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g2 = sub_x16(f3,f2);
    
    f0 = reduce_x16(qdata,f0);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_x4_16_0,zetainv_x4_qinv_16_0);
    
    g1 = add_x16(f0,f1);
    g0 = sub_x16(f0,f1);
    
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    
    _mm256_storeu_si256((__m256i *) (f + 64),f1);
    _mm256_storeu_si256((__m256i *) (f + 80),f3);
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 16),f2);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 32));
    f1 = _mm256_loadu_si256((__m256i *) (f + 96));
    f2 = _mm256_loadu_si256((__m256i *) (f + 48));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    g0 = _mm256_unpacklo_epi64(f0,f1);
    g1 = _mm256_unpacklo_epi64(f2,f3);
    g2 = _mm256_unpackhi_epi64(f0,f1);
    g3 = _mm256_unpackhi_epi64(f2,f3);
    f2 = _mm256_permute2x128_si256(g0,g1,0x31);
    f3 = _mm256_permute2x128_si256(g2,g3,0x31);
    f0 = _mm256_permute2x128_si256(g0,g1,0x20);
    f1 = _mm256_permute2x128_si256(g2,g3,0x20);
    
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_x4_32_1,zetainv_x4_qinv_32_1);
    f3 = mulmod_x16_scaled(qdata,f3,zeta_x4(32,1),zeta_x4_qinv(32,1));
    
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g2 = sub_x16(f3,f2);
    
    f0 = reduce_x16(qdata,f0);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_x4_16_1,zetainv_x4_qinv_16_1);
    
    g1 = add_x16(f0,f1);
    g0 = sub_x16(f0,f1);
    
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    
    _mm256_storeu_si256((__m256i *) (f + 96),f1);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    _mm256_storeu_si256((__m256i *) (f + 32),f0);
    _mm256_storeu_si256((__m256i *) (f + 48),f2);
    
    f += 128;
  }
  f = origf;
  for (rep = 0;rep < reps;++rep) {
    f0 = _mm256_loadu_si256((__m256i *) (f + 0));
    f2 = _mm256_loadu_si256((__m256i *) (f + 64));
    f1 = _mm256_loadu_si256((__m256i *) (f + 32));
    f3 = _mm256_loadu_si256((__m256i *) (f + 96));
    
    g0 = _mm256_permute2x128_si256(f0,f2,0x20);
    g2 = _mm256_permute2x128_si256(f0,f2,0x31);
    f0 = _mm256_unpacklo_epi16(g0,g2);
    f2 = _mm256_unpackhi_epi16(g0,g2);
    g1 = _mm256_permute2x128_si256(f1,f3,0x20);
    g3 = _mm256_permute2x128_si256(f1,f3,0x31);
    f1 = _mm256_unpacklo_epi16(g1,g3);
    f3 = _mm256_unpackhi_epi16(g1,g3);
    g1 = _mm256_unpackhi_epi16(f0,f2);
    g0 = _mm256_unpacklo_epi16(f0,f2);
    g3 = _mm256_unpackhi_epi16(f1,f3);
    g2 = _mm256_unpacklo_epi16(f1,f3);
    f2 = _mm256_unpacklo_epi64(g1,g3);
    f3 = _mm256_unpackhi_epi64(g1,g3);
    f0 = _mm256_unpacklo_epi64(g0,g2);
    f1 = _mm256_unpackhi_epi64(g0,g2);
    
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_128_0,zetainv_qinv_128_0);
    f3 = mulmod_x16_scaled(qdata,f3,zeta(128,0),zeta_qinv(128,0));
    f0 = reduce_x16(qdata,f0);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_64_0,zetainv_qinv_64_0);
    
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g0 = sub_x16(f0,f1);
    
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    
    _mm256_storeu_si256((__m256i *) (f + 32),f1);
    _mm256_storeu_si256((__m256i *) (f + 96),f3);
    _mm256_storeu_si256((__m256i *) (f + 0),f0);
    _mm256_storeu_si256((__m256i *) (f + 64),f2);
    
    f0 = _mm256_loadu_si256((__m256i *) (f + 16));
    f2 = _mm256_loadu_si256((__m256i *) (f + 80));
    f1 = _mm256_loadu_si256((__m256i *) (f + 48));
    f3 = _mm256_loadu_si256((__m256i *) (f + 112));
    
    g0 = _mm256_permute2x128_si256(f0,f2,0x20);
    g2 = _mm256_permute2x128_si256(f0,f2,0x31);
    f0 = _mm256_unpacklo_epi16(g0,g2);
    f2 = _mm256_unpackhi_epi16(g0,g2);
    g1 = _mm256_permute2x128_si256(f1,f3,0x20);
    g3 = _mm256_permute2x128_si256(f1,f3,0x31);
    f1 = _mm256_unpacklo_epi16(g1,g3);
    f3 = _mm256_unpackhi_epi16(g1,g3);
    g1 = _mm256_unpackhi_epi16(f0,f2);
    g0 = _mm256_unpacklo_epi16(f0,f2);
    g3 = _mm256_unpackhi_epi16(f1,f3);
    g2 = _mm256_unpacklo_epi16(f1,f3);
    f2 = _mm256_unpacklo_epi64(g1,g3);
    f3 = _mm256_unpackhi_epi64(g1,g3);
    f0 = _mm256_unpacklo_epi64(g0,g2);
    f1 = _mm256_unpackhi_epi64(g0,g2);
    
    f2 = mulmod_x16_scaled(qdata,f2,zetainv_128_1,zetainv_qinv_128_1);
    f3 = mulmod_x16_scaled(qdata,f3,zeta(128,1),zeta_qinv(128,1));
    f0 = reduce_x16(qdata,f0);
    f1 = mulmod_x16_scaled(qdata,f1,zetainv_64_1,zetainv_qinv_64_1);
    
    g3 = add_x16(f3,f2);
    g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
    g1 = add_x16(f0,f1);
    g2 = sub_x16(f3,f2);
    g0 = sub_x16(f0,f1);
    
    f1 = add_x16(g1,g3);
    f3 = sub_x16(g1,g3);
    f0 = add_x16(g0,g2);
    f2 = sub_x16(g0,g2);
    
    _mm256_storeu_si256((__m256i *) (f + 48),f1);
    _mm256_storeu_si256((__m256i *) (f + 112),f3);
    _mm256_storeu_si256((__m256i *) (f + 16),f0);
    _mm256_storeu_si256((__m256i *) (f + 80),f2);
    
    f += 128;
  }
}

static void invntt512(int16 *f,int reps,const __m256i *qdata)
{
  __m256i f0,f1,f2,f3,g0,g1,g2,g3,h0,h1,h2,h3;
  int16 *origf = f;
  int rep;
  __m256i zetainv_512[8];
  __m256i zetainv_qinv_512[8];
  __m256i zetainv_256[8];
  __m256i zetainv_qinv_256[8];
  int i;
  for (i = 0;i < 8;++i) zetainv_512[i] = zetainv(512,i);
  for (i = 0;i < 8;++i) zetainv_qinv_512[i] = zetainv_qinv(512,i);
  for (i = 0;i < 8;++i) zetainv_256[i] = zetainv(256,i);
  for (i = 0;i < 8;++i) zetainv_qinv_256[i] = zetainv_qinv(256,i);
  invntt128(f,4*reps,qdata);
  for (rep = 0;rep < reps;++rep) {
    for(i = 0;i < 8;++i) {
      f2 = _mm256_loadu_si256((__m256i *) (f + 16*i + 256));
      f3 = _mm256_loadu_si256((__m256i *) (f + 16*i + 384));
      
      f2 = mulmod_x16_scaled(qdata,f2,zetainv_512[i],zetainv_qinv_512[i]);
      f3 = mulmod_x16_scaled(qdata,f3,zeta(512,i),zeta_qinv(512,i));
      g3 = add_x16(f3,f2);
      g3 = mulmod_x16_scaled(qdata,g3,zeta4_x16,zeta4_x16_qinv);
      g2 = sub_x16(f3,f2);
      
      f0 = _mm256_loadu_si256((__m256i *) (f + 16*i + 0));
      f1 = _mm256_loadu_si256((__m256i *) (f + 16*i + 128));
      
      f0 = reduce_x16(qdata,f0);
      f1 = mulmod_x16_scaled(qdata,f1,zetainv_256[i],zetainv_qinv_256[i]);
      g1 = add_x16(f0,f1);
      g0 = sub_x16(f0,f1);
      
      f1 = add_x16(g1,g3);
      f3 = sub_x16(g1,g3);
      f0 = add_x16(g0,g2);
      f2 = sub_x16(g0,g2);
      
      _mm256_storeu_si256((__m256i *) (f + 16*i + 128),f1);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 384),f3);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 0),f0);
      _mm256_storeu_si256((__m256i *) (f + 16*i + 256),f2);
    }
    f += 512;
  }
}

void invntt512_7681(int16 *f,int reps)
{
  invntt512(f,reps,(const __m256i *) qdata_7681);
}

void invntt512_10753(int16 *f,int reps)
{
  invntt512(f,reps,(const __m256i *) qdata_10753);
}
